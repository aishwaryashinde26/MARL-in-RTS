## ğŸ® Multi-Agent Reinforcement Learning in Real-Time Strategy (RTS)

**Coordinated Resource Collection via Q-Learning**

### ğŸ“Œ Overview

This project implements a **Multi-Agent Reinforcement Learning (MARL)** system inspired by **Real-Time Strategy (RTS)** games. Multiple agents operate in a shared grid-based environment and **learn to collaboratively collect resources and deposit them at a base** using **tabular Q-learning** with Îµ-greedy exploration.

Despite having **no explicit communication**, agents exhibit **emergent cooperative behavior** through **reward shaping**, demonstrating how coordination can arise in decentralized systems.

The system is fully interactive, featuring:
* Real-time environment visualization
* Episode-wise and cumulative performance analytics
* Random-policy baseline comparison
* Agent-level and Q-table analysis

### âœ¨ Key Features
* ğŸ§  **Multi-Agent Q-Learning (Independent Q-Learning)**
* ğŸ¯ **Reward shaping for coordination**
* ğŸ“‰ **Îµ-greedy exploration with decay**
* ğŸ“Š **Training analytics & performance metrics**
* ğŸ® **Live RTS-style grid visualization (Streamlit + Plotly)**
* ğŸ² **Random policy baseline for comparison**
* ğŸš€ T**rain 1 Episode / Train 10 Episodes / Auto-Train modes**

### ğŸ—ï¸ Environment Description
- **Grid-based RTS environment** (default: 15Ã—15)
- **Multiple agents** (2â€“5 configurable)
- **Resource nodes** with finite quantities
- **Ally base** (collection target)
- **Enemy base** (for visualization)
- Episodes end when:
  * Maximum steps are reached (100), or
  * Required resources are deposited

The environment is **deterministic and Gym-like**, making it ideal for tabular RL experimentation.

### ğŸ§  Learning Algorithm

#### Multi-Agent Q-Learning (Independent Q-Learning)

Each agent independently maintains a Q-table and learns optimal stateâ€“action values through experience.

#### Q-Learning Update Rule
```bash
Q(s,a) â† Q(s,a) + Î± [r + Î³ Â· maxâ‚â€² Q(sâ€²,aâ€²) âˆ’ Q(s,a)]
```
Where:
- Î± â€“ Learning rate
- Î³ â€“ Discount factor
- r â€“ Immediate reward
- sâ€² â€“ Next state

### ğŸ® State & Action Space

#### State Representation


Each agentâ€™s state is encoded as:
```bash
(x, y, carrying, nearest_resource_x, nearest_resource_y, base_x, base_y)
```
This compact representation keeps the state space manageable for **tabular learning**.

#### Action Space

Agents choose from:
- up, down, left, right
- collect
- deposit

### ğŸ Reward Structure
| Event              | Reward              | Purpose                     |
| ------------------ | ------------------- | --------------------------- |
| Collect resource   | +5                  | Encourage gathering         |
| Deposit at base    | +10                 | Reward successful delivery  |
| Coordination bonus | +2 per nearby agent | Encourage teamwork          |
| Movement penalty   | -0.1                | Discourage random wandering |

â¡ï¸ **Coordination bonus** is awarded when agents deposit resources while teammates are near the base.

### ğŸ” Exploration Strategy
- **Îµ-greedy policy**
- High exploration initially
- Gradual decay:
```bash
Îµ â† max(Îµ_min, Îµ Ã— decay)
```
- Smooth transition from exploration â†’ exploitation

### ğŸ“Š Metrics & Analytics

Tracked metrics include:
- Episode reward
- Resources collected
- Success rate
- Cumulative reward
- Per-agent rewards
- Q-table size & statistics
- Action preference distribution
- Comparison with random-policy baseline

### ğŸ² Random Policy Baseline

A **10-episode random-policy evaluation** is included to validate learning effectiveness.

| Metric        | Random Policy | Learned Policy       |
| ------------- | ------------- | -------------------- |
| Avg Reward    | Very low      | Significantly higher |
| Avg Resources | 1â€“3           | 10â€“15                |
| Success Rate  | ~0%           | 70â€“100%              |

### ğŸ–¥ï¸ Visualization

The Streamlit UI provides:
- ğŸŸ© Live grid visualization
- ğŸ¤– Agent positions & status
- ğŸ’ Resource nodes with remaining quantity
- ğŸ“ˆ Training curves & analytics dashboards
- ğŸ“Š Q-value histograms & action distributions

### âš™ï¸ Technologies Used
- **Python**
- **Streamlit** â€“ UI & interaction
- **Plotly** â€“ Visualization
- **NumPy / Pandas** â€“ Computation & analytics
- **Collections (defaultdict)** â€“ Q-table storage

### ğŸš€ How to Run
1ï¸âƒ£ Install Dependencies
```bash 
pip install streamlit numpy pandas plotly
```

2ï¸âƒ£ Run the Application
```bash 
streamlit run Multi_Agent_RTS.py
```

3ï¸âƒ£ Use the UI
- Click **Initialize**
- Train using:
  - Train 1 Episode
  - Train 10 Episodes
  - Auto Train
- Compare with **Random Baseline**
- Analyze metrics and Q-table behavior

### ğŸ“Œ Limitations
- Tabular Q-learning does not scale well to large environments
- No collision or obstacle handling
- Coordination depends on reward shaping
- No explicit communication between agents
- Environment is simplified compared to full RTS games

### ğŸ”® Future Work
- Deep RL (DQN, Actor-Critic)
- Centralized Training with Decentralized Execution (CTDE)
- Inter-agent communication
- Enemy agents (cooperativeâ€“competitive setting)
- Obstacles, fog-of-war, terrain
- Curriculum & transfer learning
